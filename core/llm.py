import requests
import subprocess
import time

from core.config import RED, YELLOW, RESET

# ==================================================
# Ollama AyarlarÄ±
OLLAMA_URL = "http://localhost:11434/api/chat"
MODEL = "llama3.1:8b"
# ==================================================

SYSTEM_PROMPT = (
    "Senin adÄ±n Atlas. "
    "Bir yapay zeka asistanÄ±sÄ±n. "
    "Sadece TÃœRKÃ‡E konuÅŸ. "
    "KÄ±sa, net ve mantÄ±klÄ± cevaplar ver. "
    "Gereksiz detay, hikÃ¢ye veya yorum ekleme. "
    "Emin olmadÄ±ÄŸÄ±n konularda uydurma, bilmiyorsan aÃ§Ä±kÃ§a sÃ¶yle. "
    "CevaplarÄ±n gÃ¼nlÃ¼k ve doÄŸal TÃ¼rkÃ§e olsun. "
    "Maksimum 3-4 kÄ±sa cÃ¼mle kullan."
)

def llm_answer(msg: str, system_msg: str = None) -> str:
    # 3 kere deneme hakkÄ± veriyoruz
    max_retries = 3
    
    # EÄŸer Ã¶zel bir system prompt gelmediyse varsayÄ±lanÄ± kullan
    final_system_prompt = system_msg if system_msg else SYSTEM_PROMPT

    for i in range(max_retries):
        try:
            payload = {
                "model": MODEL,
                "messages": [
                    {"role": "system", "content": final_system_prompt},
                    {"role": "user", "content": msg}
                ],
                "stream": False
            }

            # Timeout sÃ¼resini artÄ±rdÄ±k Ã§Ã¼nkÃ¼ modelin yÃ¼klenmesi uzun sÃ¼rebilir
            r = requests.post(OLLAMA_URL, json=payload, timeout=180)
            r.raise_for_status()

            data = r.json()
            return data["message"]["content"]

        except Exception as e:
            print(RED + f"[OLLAMA HATASI - Deneme {i+1}/{max_retries}] {e}")
            if "500" in str(e) or "Connection refused" in str(e):
                print(f"{YELLOW}â³ VRAM'in boÅŸalmasÄ± bekleniyor (5 sn)...{RESET}")
                time.sleep(5)  # 5 saniye bekle ve tekrar dene
            else:
                # BaÅŸka bir hataysa (Ã¶rn: internet yok) bekleme, direkt Ã§Ä±k
                break
    
    return "Åžu an cevap veremiyorum (Teknik arÄ±za)."


def ollama_warmup():
    """
    Ollama modelini Atlas baÅŸlamadan Ã¶nce RAM/GPU'ya yÃ¼kler.
    Offline modda 500 hatasÄ±nÄ± Ã¶nler.
    """
    try:
        print("ðŸ§  Ollama modeli Ä±sÄ±tÄ±lÄ±yor (warm-up)...")
        subprocess.Popen(
            ["ollama", "run", MODEL],
            stdin=subprocess.PIPE,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        time.sleep(2.5)
        print("âœ… Ollama warm-up tamamlandÄ±.")
    except Exception as e:
        print(f"âš ï¸ Ollama warm-up baÅŸarÄ±sÄ±z: {e}")


# llm.py dosyasÄ±nÄ±n en altÄ±na ekle:

def unload_ollama():
    """
    Ollama modelini VRAM'den zorla boÅŸaltÄ±r.
    BÃ¶ylece Stable Diffusion iÃ§in yer aÃ§Ä±lÄ±r.
    """
    try:
        # keep_alive: 0 parametresini gÃ¶nderince model hemen unload olur
        payload = {"model": MODEL, "keep_alive": 0}
        requests.post(OLLAMA_URL, json=payload, timeout=3)
        print(f"{RED}ðŸ§¹ Ollama VRAM'den temizlendi.{RESET}")
    except Exception as e:
        print(f"âš ï¸ VRAM temizleme hatasÄ±: {e}")

def visual_prompt_generator(user_text: str) -> str:
    """
    KullanÄ±cÄ±nÄ±n girdiÄŸi (muhtemelen TÃ¼rkÃ§e) metni, 
    Stable Diffusion iÃ§in uygun Ä°NGÄ°LÄ°ZCE prompt haline getirir.
    """
    system_msg = (
        "You are a world-class AI Art Director and Prompt Engineer known for creating 'Sora-level' realism. "
        "Your task: Convert the user's input (in Turkish) into a BREATHTAKING, CINEMATIC, and HYPER-REALISTIC English image prompt. "
        "Rules:\n"
        "1. Translate the core concept but ELEVATE it to a blockbuster movie scene.\n"
        "2. REQUIRED KEYWORDS: 'Award-winning photography, 8k raw photo, soft cinematic lighting, extremely detailed, Unreal Engine 5 render, sharp focus, 85mm lens, f/1.8, bokeh'.\n"
        "3. STYLE: Hyper-realism, Documentary, National Geographic, IMAX quality.\n"
        "4. AVOID: 'Cartoon, illustration, 3d render looking, painting, drawing, low resolution'.\n"
        "5. Output ONLY the prompt string.\n"
        "6. Example: 'sarÄ± araba' -> 'A hyper-realistic 8k shot of a yellow sports car drifting on a rainy asphalt road, reflection of neon city lights, cinematic lighting, dramatic atmosphere, shot on 35mm film, award-winning photography.'"
    )
    
    try:
        payload = {
            "model": MODEL,
            "messages": [
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_text}
            ],
            "stream": False
        }
        r = requests.post(OLLAMA_URL, json=payload, timeout=60)
        r.raise_for_status()
        
        prompt_en = r.json()["message"]["content"].strip()
        
        # Temizlik
        if ":" in prompt_en and len(prompt_en.split(":")[0]) < 20: # "Detailed prompt: ..." gibi ÅŸeyleri temizle
            prompt_en = prompt_en.split(":")[-1].strip()
            
        return prompt_en
        
    except Exception as e:
        print(f"Prompt Generation Error: {e}")
        # Hata olursa en azÄ±ndan orijinalini (veya basit Ã§eviriyi) dÃ¶ndÃ¼rmeye Ã§alÄ±ÅŸalÄ±m 
        # ama LLM yoksa yapacak bir ÅŸey yok, orijinali yolla.
        return user_text